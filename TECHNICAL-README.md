
# üöÄ Bootcamp Modern Data Stack Architecture

This bootcamp uses a **lightweight Modern Data Stack (MDS)** to teach the **Extract ‚Üí Load ‚Üí Transform ‚Üí Visualize** workflow on real datasets.

## Local Setup

```mermaid
flowchart LR
  subgraph Extract-Load
    direction LR
    SRC[(External<br>data sources)]
    DLT[dlt&nbsp;container]
    SRC --> DLT
    DLT -->|raw tables| CH[(ClickHouse)]
  end
  subgraph Transform
    direction LR
    DBT[dbt&nbsp;container]
    CH --> DBT
    DBT -->|views / tables| CH
  end
  subgraph Visualise
    direction LR
    CH --> META[Metabase UI]
  end

  classDef box fill:#d6eaf8,stroke:#036,stroke-width:1px,color:#000;
  class SRC,DLT,DBT,META box;
  style CH fill:#fde9b4,stroke:#663,stroke-width:2px,color:#000
```

## Remote Setup

```mermaid
flowchart LR
  %% STYLE DEFINITIONS
  classDef box fill:#d6eaf8,stroke:#036,stroke-width:1px,color:#000;
  classDef remote fill:#fde9b4,stroke:#663,stroke-width:2px,color:#000;

  %% === LOCAL MACHINE ===
  subgraph LOCAL[Local machine]
    direction LR
    SRC[(External data sources)]
    DLT[dlt local]
    DBT[dbt local]
    class SRC,DLT,DBT box
  end

  %% === REMOTE SERVER ===
  subgraph REMOTE[Remote server]
    direction LR
    CH[(ClickHouse)]
    META[Metabase UI]
    class CH,META remote
  end

  %% FLOWS
  SRC --> DLT
  DLT -->|raw tables| CH
  CH --> DBT
  DBT -->|views / tables| CH
  CH --> META
```


## ‚öôÔ∏è Components & Roles

| Layer             | Tool                 | Role in the Stack                                               |
| ----------------- | -------------------- | --------------------------------------------------------------- |
| **Ingestion**     | `dlt`                | Connectors for APIs, CSVs, scraping, DB; retries; deduplication |
| **Storage**       | `ClickHouse`         | Fast columnar OLAP warehouse; MergeTree engines for time-series |
| **Transform**     | `dbt`                | SQL models, schema tests, semantic layer (facts & dims)         |
| **Visualization** | `Metabase`           | Self-service BI dashboards                                      |
| **Source DB**     | `Postgres (Chinook)` | Example operational system for ingestion demo                   |

---
* **Sources**: Static CSV, Transactional Database, Timeseries API, Streaming API, Static API, Ecommerce Website, Other sources.
* **dlt (Extract & Load)**: Ingests data from multiple formats/APIs, applies light schema, pushes to warehouse.
* **ClickHouse (Warehouse)**: Stores data in efficient **MergeTree**/ **ReplacingMergeTree** tables with partitions (by date/region) for fast analytics.
* **dbt (Transform)**: Builds **staging ‚Üí core ‚Üí marts** models, runs tests (`unique`, `not_null`), and materializes facts/dimensions.
* **Metabase (Visualize)**: Queries the warehouse and builds dashboards (weather, BTC prices, reviews).


> **Note:** In production you‚Äôd add a data lake (e.g. S3/Delta Lake), data catalog (Amundsen/DataHub), security/Governance (RBAC, encryption, Ranger), orchestration (Airflow/Kubernetes), observability (Prometheus/Grafana), and data quality (Great Expectations). We‚Äôve omitted those to focus on core ELT.

Above all, this bootcamp emphasizes **SQL** with only minimal Python, YAML, and Markdown.


---


## üóÑÔ∏è Modeling Conventions

* **dlt ‚Üí raw layer**: Tables named `raw_*`, append/merge mode.
* **dbt ‚Üí clean**: `dim_*` and `fct_*` models with tests.
* **dbt ‚Üí marts**: Aggregations, rollups, and reporting views.
* **exploratory -> sandbox**: Discovery and temporary tables.

---

## üîí Security & Isolation

* Students connect with **`ftw_user`** (no `CREATE DATABASE`).
* Each student `_<alias>` to tables and names to avoid collisions.
* Secrets (API keys, passwords) are stored in **dlt secrets.toml** or `.env`.

---

## üìä Example Pipelines

* `01_mpg.py` ‚Üí UCI Auto MPG dataset
* `02_chinook.py` ‚Üí Artists & Albums from Postgres
* `03_meteo_*` ‚Üí Historical weather + PH regions + WMO codes
* `04_btc_markets.py` ‚Üí CoinGecko Bitcoin prices
* `05_pokeapi_gen1.py` ‚Üí Pok√©mon API (Gen 1)
* `06_lazada_scrape.py` ‚Üí Lazada scraping (Playwright)
* `07_foodpanda.py` ‚Üí Kaggle restaurant reviews

---

## ‚úÖ Key Teaching Points

* How to ingest from **databases, APIs, CSVs, scrapers**.
* How **ClickHouse engines** affect deduplication, partitions, and performance.
* Why **dbt tests** matter (data quality + schema enforcement).
* How to isolate schemas/users for **multi-tenant teaching environments**.
* Building quick **dashboards** in Metabase from dbt models.

---

## üìå Extensions (for production mindset)

* Add **orchestration** (Mage/Dagster/Airflow) to chain dlt ‚Üí dbt ‚Üí tests.
* Introduce **data contracts** (typed schemas, versioning).
* Enable **observability** (OpenLineage, Marquez, or dbt docs lineage).
* Apply **monitoring/alerts** on late data and API failures.

---

üëâ This architecture balances **simplicity (bootcamp teaching)** with **modern practices** (idempotent loads, schema tests, semantic layers, partitioning). Learners get a hands-on feel for the modern data stack without the operational overhead of enterprise setups.
 

# FTW DE Bootcamp ‚Äî Exercise Run Guide

This guide shows how to run each **extract ‚Üí load (dlt)** and **transform (dbt)** exercise in your Docker setup. Commands assume you‚Äôre in the project root.

> **Quick tips**
>
> * Start the **core** stack once (ClickHouse + Metabase + Chinook Postgres) and leave it running.
> * Each exercise is: **run dlt ‚Üí run dbt (in that exercise folder)**.
> * Local vs Remote: edit envs in `compose.yaml` as noted in each service block.

---

## 0) Start/Stop core services

```bash
# Start core (ClickHouse, Metabase, Chinook Postgres)
docker compose --profile core --compatibility up -d

# Check health
docker ps
# Metabase: http://localhost:3001 ‚Ä¢ ClickHouse HTTP: http://localhost:8123

# Stop everything (when done)
docker compose down
```

---

## How to reset docker containers

### stop Metabase only
docker compose stop metabase

### remove the metabase container and any anonymous volumes it owns
docker compose rm -f -s -v metabase

### find the actual named volume (compose prefixes it with the project name)
docker volume ls | grep metabase_data
#### e.g. myproject_metabase_data

## delete that named volume (this wipes metabase.db and all Metabase state)
docker volume rm <yourproject>_metabase_data

### bring Metabase back fresh
docker compose up -d metabase


---

## 01 ‚Äî Auto MPG

**Extract & Load (dlt)**

```bash
docker compose --profile jobs run --rm \
  dlt python extract-loads/01-dlt-mpg-pipeline.py
```

**Transform (dbt)**

```bash
docker compose --profile jobs run --rm \
  -w /workdir/transforms/01_mpg \
  dbt build --profiles-dir . --target local
```

**Verify**

* In Metabase, add ClickHouse (if not added) and browse the `clean` / `mart` models for MPG.

---

## Did you encounter an ingestion error?
If for some reason the tables do not exist and throws an error, do the following:

### Option A ‚Äî one-time manual create (safe, non-destructive)

Just create the DBs now and move on:

```bash
docker exec -it clickhouse clickhouse-client -q "
  CREATE DATABASE IF NOT EXISTS raw;
  CREATE DATABASE IF NOT EXISTS clean;
  CREATE DATABASE IF NOT EXISTS mart;
  CREATE DATABASE IF NOT EXISTS sandbox;
"
```

You can also run this via dbeaver.

### Option B ‚Äî reinitialize the container (will wipe ClickHouse data)

If you actually want the entrypoint to re-run the init scripts (and you‚Äôre okay losing anything in `clickhouse_data`):

```bash
# Stop and remove containers + volumes for this compose project
docker compose down -v

# (Optional) double-check the volume is gone:
docker volume ls | grep clickhouse_data

# Bring it back up; first boot will now run init scripts
docker compose --compatibility up -d --profile core

# Verify
docker exec -it clickhouse clickhouse-client -q "SHOW DATABASES"
```

---

## 02 ‚Äî Chinook (Postgres ‚Üí ClickHouse)

> Chinook Postgres is prepared by `chinook_fetch` + `postgres_chinook` in the **core** profile.

**Extract & Load (dlt)**

```bash
docker compose --profile jobs run --rm \
  dlt python extract-loads/02-dlt-chinook-pipeline.py
```

**Transform (dbt)**

```bash
docker compose --profile jobs run --rm \
  -w /workdir/transforms/02_chinook \
  dbt build --profiles-dir . --target local
```

**Verify**

* Explore albums, artists, invoices in Metabase; confirm `stg_` views and `mart_` models appear.

---

## 03 ‚Äî Weather (Meteo)

There are **three** loaders you can try:

* `03-dlt-meteo-pipeline.py` (base daily history)
* `03-dlt-xtra-meteo-ph-regions.py` (regional add-ons)
* `03-dlt-wmo-codes-pipeline.py` (lookup codes)

**Extract & Load (dlt)**

```bash
# Base
docker compose --profile jobs run --rm \
  dlt python extract-loads/03-dlt-meteo-pipeline.py

# Optional extras
docker compose --profile jobs run --rm \
  dlt python extract-loads/03-dlt-xtra-meteo-ph-regions.py

docker compose --profile jobs run --rm \
  dlt python extract-loads/03-dlt-wmo-codes-pipeline.py
```

**Transform (dbt)**

```bash
docker compose --profile jobs run --rm \
  -w /workdir/transforms/03_meteo \
  dbt build --profiles-dir . --target local
```

**Verify**

* Check cleaned daily temps/precip, join to WMO code lookups, and sample dashboards in Metabase.

---

## 04 ‚Äî Bitcoin (BTC)

Note: make a non-scheduled version

**Extract & Load (dlt)**

```bash
docker compose --profile jobs run --rm \
  dlt python extract-loads/04-dlt-btc-pipeline.py
```

**Transform (dbt)**

```bash
docker compose --profile jobs run --rm \
  -w /workdir/transforms/04_btc \
  dbt build --profiles-dir . --target local
```

**Verify**

* Plot OHLC/close vs date in Metabase. Try simple moving averages in a SQL question.

---

## 05 ‚Äî Pok√©mon

**Extract & Load (dlt)**

```bash
docker compose --profile jobs run --rm \
  dlt python extract-loads/05-dlt-poke-pipeline.py
```

**Transform (dbt)**

```bash
docker compose --profile jobs run --rm \
  -w /workdir/transforms/05_poke \
  dbt build --profiles-dir . --target local
```

**Verify**

* Browse Pok√©mon stats/models in Metabase; create a bar chart by type/avg attack.

---

## 06 ‚Äî Lazada (sample dataset)

**Extract & Load (dlt)**

```bash
docker compose --profile jobs run --rm \
  dlt python extract-loads/06-dlt-laz-pipeline.py
```

**Transform (dbt)**

```bash
docker compose --profile jobs run --rm \
  -w /workdir/transforms/06_laz \
  dbt build --profiles-dir . --target local
```

**Verify**

* Build a price vs rating scatter, category leaders, top sellers, etc.

---

## 07 ‚Äî Foodpanda Reviews (Optional)

**Extract & Load (dlt)**

```bash
docker compose --profile jobs run --rm \
  dlt python extract-loads/07-dlt-food-pipeline.py
```

**Transform (dbt)**

> ‚ö†Ô∏è Your snippet pointed dbt to `06_laz`. The correct path is **`07_foodpanda`**.

```bash
docker compose --profile jobs run --rm \
  -w /workdir/transforms/07_foodpanda \
  dbt build --profiles-dir . --target local
```

**Verify**

* Sentiment distributions, top restaurants by average rating and review volume.

---

## 08 ‚Äî Unstructured (Optional)

**Extract & Load (dlt)**

```bash
docker compose --profile jobs run --rm \
  dlt python extract-loads/08-dlt-unstructured-pipeline.py
```

**Transform (dbt)**
(If you add a folder later under `dbt/transforms/08_unstructured`)

```bash
docker compose --profile jobs run --rm \
  -w /workdir/transforms/08_unstructured \
  dbt build --profiles-dir . --target local
```

---

## Remote Mode (students target a server)

1. On the **server** (teacher):

   ```bash
   docker compose --compatibility up -d --profile core
   # Ensure inbound ports 8123 (ClickHouse HTTP) and 3001 (Metabase) are open.
   ```
2. On **student** laptops:

   * In `compose.yaml`:

     * For **dlt**: set `DESTINATION__CLICKHOUSE__CREDENTIALS__HOST` to server IP/DNS
     * For **dbt**: set `CH_HOST` to server IP/DNS`
   * Then run the same **dlt** and **dbt** commands per exercise.

---

## Troubleshooting Cheatsheet

* **ClickHouse not ready**

  * Wait for healthcheck: `docker logs clickhouse`
* **Chinook loader not found**

  * Ensure `chinook_fetch` completed and `postgres_chinook` is healthy: `docker ps`

* **Metabase can‚Äôt see new tables**

  * In Admin ‚Üí Databases ‚Üí ClickHouse ‚Üí **Sync** & **Re-scan field values**

---

## Clean Up

```bash
# Stop and remove containers (keep volumes)
docker compose down

# Nuke volumes (‚ö†Ô∏è deletes data)
docker compose down -v
```

That‚Äôs it‚Äîrun dlt then dbt for each folder above, and explore your models in Metabase.
